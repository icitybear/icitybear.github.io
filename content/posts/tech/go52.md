---
title: "go包-限流/限频" #标题
date: 2024-07-12T14:40:10+08:00 #创建时间
lastmod: 2024-07-12T14:40:10+08:00 #更新时间
author: ["citybear"] #作者
categories: # 没有分类界面可以不填写
- tech
tags: # 标签
- go包
- go源码
keywords: 
- 
description: "" #描述 每个文章内容前面的展示描述
weight: # 输入1可以顶置文章，用来给文章展示排序，不填就默认按时间排序
slug: ""
draft: false # 是否为草稿
comments: true #是否展示评论 有自带的扩展成twikoo
showToc: true # 显示目录 文章侧边栏toc目录
TocOpen: true # 自动展开目录
hidemeta: false # 是否隐藏文章的元信息，如发布日期、作者等
disableShare: true # 底部不显示分享栏
showbreadcrumbs: true #顶部显示当前路径
cover:
    image: "" #图片路径：posts/tech/文章1/picture.png
    caption: "" #图片底部描述
    alt: ""
    relative: false

# reward: true # 打赏
mermaid: true #自己加的是否开启mermaid
---
流量控制Q 基本是 《微服务》和高并发系统设计的入门课，即便是在早期各种负载均衡和网络组件 （如nginx、iptable、TC）都有提供基础的QPS限制能力，如今演进到微服务框架、<font color="red">Sentinel</font>、Service Mesh和Serverless都已经具备完备的配置化的限流的能力已经能够满足大多数场景了

# 信号量semaphor包和channel最大并发数
{{< innerlink src="posts/tech/go65.md" >}}

# 传统限流
<font color="red">在一些不以服务作为颗粒的方式 比如以下几个场景：</font>
1. 调用外部三方服务存在频率限制
2. 队列消费控速 。
   - 设置最大消费线程数、每次拉取消息条数拉取间隔时间 。
   - 这样能够能精准控制队列的处理频率吗？（有些主流MQ中间件SDK有实现匀速器，如RocketMQ，但是单点限流不能多消费者 分布式共享） 
3. 当被超过频率限制时执行一段兜底逻辑

## 固定窗口算法
![alt text](image1.png)

``` go
// FixedWindowCounter 固定窗口计数器限流器
type FixedWindowCounter struct {
	limit    int64         // 限制数量
	window   time.Duration // 时间窗口
	counter  int64         // 当前计数
	lastTime time.Time     // 上次重置时间
	mutex    sync.Mutex    // 互斥锁
}

// NewFixedWindowCounter 创建固定窗口计数器
func NewFixedWindowCounter(limit int64, window time.Duration) *FixedWindowCounter {
	return &FixedWindowCounter{
		limit:    limit,
		window:   window,
		counter:  0,
		lastTime: time.Now(),
	}
}

func (f *FixedWindowCounter) Allow() bool {
	f.mutex.Lock()
	defer f.mutex.Unlock()

	now := time.Now()
	// 如果超过这段时间范围，重置计数器
	if now.Sub(f.lastTime) >= f.window {
		f.counter = 0
		f.lastTime = now
	}
	// 检查是否超过限制
	if f.counter >= f.limit {
		return false
	}
	f.counter++
	return true
}
```
- 缺点：在临界点上，如果突然增加了流量，会导致请求到下游的流量翻倍（自然秒窗口导致0.9s超限）

## redis版固定窗口
- go-zero的计数器包 limit.PeriodLimit代码参考
``` go
var (
    // 表示未知状态码的错误。
    ErrUnknownCode = errors.New("unknown status code")
)
const (
    // 未初始化状态。
    Unknown = iota
    // 允许状态。
    Allowed
    // 准确达到配额。
    HitQuota
    // 超过配额。
    OverQuota
)

// period: 限制应用的时间段，以秒为单位。
// quota: 在指定时间段内允许的最大请求数量。
// limitStore: 用于跟踪限制的Redis存储实例。
// keyPrefix: 用于区分不同限制的键前缀。
// opts: 自定义PeriodLimit实例的可选参数。
func NewPeriodLimit(period, quota int, limitStore *redis.Redis, keyPrefix string,
   opts ...PeriodOption) *PeriodLimit {
   limiter := &PeriodLimit{
      period:     period,
      quota:      quota,
      limitStore: limitStore,
      keyPrefix:  keyPrefix,
   }

   for _, opt := range opts {
      opt(limiter)
   }

   return limiter
}

// TakeCtx 也有不带参数的Take
func (h *PeriodLimit) TakeCtx(ctx context.Context, key string) (int, error) {
   resp, err := h.limitStore.ScriptRunCtx(ctx, periodScript, []string{h.keyPrefix + key}, []string{
      strconv.Itoa(h.quota),
      strconv.Itoa(h.calcExpireSeconds()),// 直接调用了 lua 脚本 还使用了函数 calcExpireSeconds
   })
   if err != nil {
      return Unknown, err
   }

   code, ok := resp.(int64)
   if !ok {
      return Unknown, ErrUnknownCode
   }

   switch code {
   case internalOverQuota:
      return OverQuota, nil
   case internalAllowed:
      return Allowed, nil
   case internalHitQuota:
      return HitQuota, nil
   default:
      return Unknown, ErrUnknownCode
   }
}

// 代码中的关键部分是unix%int64(h.period)，这部分代码对当前时间戳unix取模h.period。取模运算的结果是一个介于0和h.period-1之间的数，这个数表示当前时间在h.period时间周期中的位置。然后，通过从h.period中减去这个结果，可以得到一个与h.period有关的偏移量。这个偏移量表示了当前时间距离下一个完整的h.period周期还有多少时间
func (h *PeriodLimit) calcExpireSeconds() int {
   if h.align {
      now := time.Now()
      _, offset := now.Zone()
      unix := now.Unix() + int64(offset)
      return h.period - int(unix%int64(h.period))
   }

   return h.period
}

// -------------------------------------参考看门狗------//
// periodScript = `` // 以下字符串内容 
// incrby + 过期时间 来做的 incr 命令使用之后 会返回当前的值
local limit = tonumber(ARGV[1])
local window = tonumber(ARGV[2])
local current = redis.call("INCRBY", KEYS[1], 1) // 每次current + 1 
if current == 1 then
    redis.call("expire", KEYS[1], window) // 限流使用的是 // 第一次调用 incrby 的时候给加一个窗口 
end
if current < limit then  // current 与 limit 的关系
    return 1
elseif current == limit then
    return 2
else
    return 0
end
// -------------------------------------参考看门狗------//
```

## 滑动窗口
![alt text](image2.png)
``` go
type SlidingWindowLimiter struct {
	windowDuration int64        // 窗口总时长(单位：毫秒)
	maxRequests    int64        // 窗口内允许的最大请求数
	slices         []*TimeSlice // 时间片集合（按时间排序）
	head           int          // 有效时间片的起始索引
	// tail           int          // 有效时间片的结束索引
	totalRequests int64      // 当前窗口内总请求数
	mutex         sync.Mutex // 并发控制锁
}

func NewSlidingWindowLimiter(limit int64, window int64, precision int64) *SlidingWindowLimiter {
	if precision <= 0 {
		precision = 1
	}

	return &SlidingWindowLimiter{
		maxRequests:    limit,
		windowDuration: window,
		slices:         make([]*TimeSlice, 0, window/precision), // 预分配空间
		mutex:          sync.Mutex{},
	}
}

func (l *SlidingWindowLimiter) Allow() bool {
	l.mutex.Lock()
	defer l.mutex.Unlock()

	now := time.Now().UnixNano() / 1e6 // 当前毫秒时间戳
	cutoff := now - l.windowDuration   // 过期时间点

	// 清理过期时间片（从头部开始）
	for l.head < len(l.slices) {
		if l.slices[l.head].timestamp >= cutoff {
			break
		}
		l.totalRequests -= l.slices[l.head].count
		l.head++
	}

	// 如果头部指针超过切片一半，则压缩切片
	if l.head > len(l.slices)/2 {
		l.slices = append(make([]*TimeSlice, 0, cap(l.slices)), l.slices[l.head:]...)
		l.head = 0
	}

	// 检查是否超过限制
	if l.totalRequests >= l.maxRequests {
		return false
	}

	// 尝试更新最近的时间片
	if len(l.slices) > l.head {
		last := l.slices[len(l.slices)-1]
		if last.timestamp == now {
			last.count++
			l.totalRequests++
			return true
		}
	}

	// 添加新的时间片
	l.slices = append(l.slices, &TimeSlice{
		timestamp: now,
		count:     1,
	})
	l.totalRequests++
	return true
}
```

- <font color="red">滑动窗口可以很平滑地控制流量，通过精准动态窗口解决自然秒窗口导致0.9s超限，结合缓冲channel实现，应对突发的流量</font>
- 缺点： 但如果这个窗口如果一开始就被打满了，那么剩下的时间都不可访问了
![alt text](image3.png)

## 漏桶 go.uber.org/ratelimit
- 漏桶算法也具备削峰填谷的作用，经过漏桶后请求就能匀速平滑的流出 类似MQ消息队列 为了保护下游，避免下游打挂，下游不能承受过重的QPS，只能一点一点地喂给下游
  1. 启动一个协程定时漏水
  2. 如果桶满了，不再给予请求通过，如果没满，则放入桶中等待处理请求
  3. 获取桶中的水滴后，对桶内的容量进行扣除，以让下一个请求进行处理

- <font color="red">缺点：无法应对突发流量的来袭，并且处理请求会有延迟</font>
例子：对于http请求我们必须要求低时延，不能说我一个请求要等很久，并且漏桶算法也没法处理突发流量，所以就引入了令牌桶算法。

``` go
func TestRateLimit(t *testing.T) {
    limiter := ratelimit.New(50) // 1.漏桶速率50

    size := 500
    data := generateData(size)

    var wg sync.WaitGroup
    startTime := time.Now()
    for i, item := range data {
        wg.Add(1)
        go func(idx int, obj any) {
            defer wg.Done()
            limiter.Take() // 2. 入桶待漏
            processed, err := process(obj)
            if err != nil {
                t.Logf("[%d] [ERROR] processed: %v, err: %v", idx, processed, err)
            } else {
                t.Logf("[%d] [OK] processed: %v", idx, processed)
            }
        }(i, item)
    }
    wg.Wait()
    endTime := time.Now()
    t.Logf("start: %v, end: %v, seconds: %v", startTime, endTime, endTime.Sub(startTime).Seconds())
}

```

## 令牌桶 golang.org/x/time/rate

- 漏桶的进一步优化
  - 容纳了 burst 个令牌，可以提供临时的突发份额。 如果不能容忍突发，漏桶更加合适。
  - <font color="red">速度 T token/秒, 容量最大为 Burst 个, 尝试取出 N 个 token, N <= burst剩余 token 不足 则等待</font> 解决延迟
- 令牌桶算法：在特定容量的桶里面装令牌，当令牌数量小于桶的容量时，会持续以我们预期的限流速率生产令牌；不管桶里面是不是 空的，业务都得等到拿到令牌，才能继续执行业务逻辑
- 业务通过令牌桶方式限速时，<font color="red">如果需要限制冷启动时的瞬时速率，需要留意把burst的值设置小一些</font>
  - burst这个桶具备了缓冲作用，在冷启动时，由于burst的存在，初始的QPS会比实际预估的较大

- <font color="red">限流值和 take 数量相差较大，将会导致意料外的长时间阻塞</font>
  - 阻塞请求本身
  - 限流器内部长时间地持有锁，也会导致更新操作阻塞异常

比如限流 1KB/s，请求 10MB，下一个请求将阻塞 ~1000s。更新限流值也将在 1000s 后才能生效

![alt text](image4.png)
``` go
import "golang.org/x/time/rate" // 需要import的rate库，其它import暂时忽略

// 生成0->X的数据集
func generateData(num int) []any {
    var data []any
    for i := 0; i < num; i++ {
        data = append(data, i)
    }
    return data
}

// 处理数据，数字*10
func process(obj any) (any, error) {
    integer, ok := obj.(int)
    if !ok {
        return nil, errors.New("invalid integer")
    }
    time.Sleep(1)
    nextInteger := integer * 10
    if integer%99 == 0 {
        return nextInteger, errors.New("not a happy number")
    }
    return nextInteger, nil
}

func TestRate(t *testing.T) {
    limit := rate.Limit(5) // QPS：5 基础速率
    burst := 25 // 桶容量25
    limiter := rate.NewLimiter(limit, burst) // 1. 初始化一个令牌生成速率为limit，容量为burst的令牌桶
    
    size := 50 // 数据量50
    data := generateData(size)

    var wg sync.WaitGroup // 工作组锁
    startTime := time.Now()
    for i, item := range data {
        wg.Add(1)
        go func(idx int, obj any) {
            defer wg.Done()
            // 2. Wait拿到令牌
            if err := limiter.Wait(context.Background()); err != nil {
                t.Logf("[%d] [EXCEPTION] wait err: %v", idx, err)
            }
            // 执行业务逻辑
            processed, err := process(obj)
            if err != nil {
                t.Logf("[%d] [ERROR] processed: %v, err: %v", idx, processed, err)
            } else {
                t.Logf("[%d] [OK] processed: %v", idx, processed)
            }
        }(i, item)
    }
    wg.Wait()
    endTime := time.Now()
    t.Logf("start: %v, end: %v, seconds: %v", startTime, endTime, endTime.Sub(startTime).Seconds())
}
```

- [Go 限流控制《滑动窗口&令牌桶》：time/rate、TokenLimit、PeriodLimit](https://blog.csdn.net/u011142688/article/details/126683615)
  - 保证原子性基于redis实现分布式令牌桶，lua脚本
  - go-zero提供的令牌桶当redis故障是会切换到内存令牌桶time/rate来降低影响，但是会影响限流的精准性

### 理解qps和初始化的容量，合理使用ctx
``` go

func TestTimeRate(t *testing.T) {

	num := rate.Every(time.Millisecond * 100) // 0.1=>每秒10个 100写成500每秒50个
	// num := rate.Limit(10)                // 10=>0.1（每秒10个） 20 => 0.2
	limiter := rate.NewLimiter(num, 5) // 1. 初始化一个令牌生成速率为limit，容量为burst的令牌桶
	// tag:容量为burst容量一开始就满了，然后下个时间频率继续产令牌
	timer := time.NewTimer(time.Second * 3) // 定时器 5秒的定时器 5s后通道收到消息
	quit := make(chan struct{})             // 通道
	defer timer.Stop()
	go func() {
		<-timer.C   // 定时器到期 定时器收到消息
		close(quit) // 通知子协程都关闭
	}()

	var allowed, denied int32
	var wait sync.WaitGroup
	cpuNum := runtime.NumCPU()
	fmt.Println(cpuNum) // 8核

	for i := 0; i < cpuNum; i++ {
		wait.Add(1)
		// 1秒能循环1万次
		go func() {
			
			for {
				select {
				// tag:接收外部关闭消息
				case <-quit:
					wait.Done()
					return
				default:
					ctx, cancel := context.WithTimeout(context.TODO(), time.Second) // 1s超时
					// 2. Wait拿到令牌
					err := limiter.Wait(ctx)
					if err == nil {
						// tag: 能证明初始化就有burst个， 令牌设置成1s1个，然后allowed是变量外部传入的 打印结果 0 1 2 3 0 5 6 7
						t.Logf("获取到令牌: allowed %v, Time: %v", allowed, time.Now().Format("2006-01-02 15:04:05.000")) //打印每个抢到的时间
						atomic.AddInt32(&allowed, 1)
					} else {
						//tag: 如果令牌产生太慢就会有N个这日志 ctx设置1秒超时 1s都有1万次 rate: Wait(n=1) would exceed context deadline
						t.Logf("未获取到令牌:denied %v, Time: %v, err: %s", denied, time.Now().Format("2006-01-02 15:04:05.000"), err.Error())
						fmt.Println(err)
						atomic.AddInt32(&denied, 1)
					}
					cancel()
				}
			}

		}()
	}

	wait.Wait()
	fmt.Printf("allowed: %d, denied: %d, qps: %d\n", allowed, denied, (allowed+denied)/10)
}

// 8
//rate_test.go:109: 获取到令牌: allowed 0, Time: 2024-08-07 18:29:34.213
//rate_test.go:109: 获取到令牌: allowed 1, Time: 2024-08-07 18:29:34.213
//rate_test.go:109: 获取到令牌: allowed 2, Time: 2024-08-07 18:29:34.213
//rate_test.go:109: 获取到令牌: allowed 3, Time: 2024-08-07 18:29:34.213
//rate_test.go:109: 获取到令牌: allowed 0, Time: 2024-08-07 18:29:34.213 //初始化的容量就是5个
//rate_test.go:109: 获取到令牌: allowed 5, Time: 2024-08-07 18:29:35.213 //然后频率设置1秒1个令牌
//rate_test.go:109: 获取到令牌: allowed 6, Time: 2024-08-07 18:29:36.213
//rate_test.go:109: 获取到令牌: allowed 7, Time: 2024-08-07 18:29:37.213
```

### 实战 控制第三方请求qps 

``` go
// QqPromotionNormDayReportLoad 广告组每日报表-各种指标
func (q *Qq3ReportCase) QqPromotionNormDayReportLoad(ctx context.Context, startDate string) {
	bT := time.Now()
	log.Context(ctx).Infof("[QqPromotionNormDayReportLoad]: Run start:%+v", bT)
    // 1. 方便本地模拟跑数据
	advertisers, _ := data.NewQqAdvertiserListV3Repo().RecordGetAll(ctx, &models.QqAdvertiserListV3ListParam{}, "advertiser_id,majordomo_id", "id asc")

	if len(advertisers) <= 0 {
		log.Context(ctx).Warnf("[QqPromotionNormDayReportLoad]#没有查询到广告主")
		return
	}

	// 令牌桶 如果是分布式就要考虑redis的lua脚本，如果api是全局也要考虑
	// rate.Every(time.Millisecond * 100) 1个的时间=》1ms*100 也就是每秒10个
	limit := rate.Limit(33)
	limiter := rate.NewLimiter(limit, 100) // 2. 限流器的最大事件频率和一次可以消费的最大令牌数
	wg := sync.WaitGroup{} // 工作组，等待所有goroutine都执行完毕
	// 3. tag: 1.通过信号量控制通知进行的最大并发 2.可以分页取广告主
	concurrent := semaphore.NewWeighted(5)
	for _, advertiserInfo := range advertisers {
		concurrent.Acquire(ctx, 1)
		wg.Add(1)
		log.Context(ctx).Infof("[QqPromotionNormDayReportLoad]#syncPullGdtAdGroupDayReport advertiser: %+v, majordomo: %+v", advertiserInfo.AdvertiserID, advertiserInfo.MajordomoID)
		// wg concurrent 看是否写到go func(闭包参数列表){}(实参数)
        go q.syncPullGdtAdGroupDayReport(ctx, advertiserInfo.AdvertiserID, startDate, &wg, limiter, concurrent)
	}

	wg.Wait()

	eT := time.Since(bT)
	log.Context(ctx).Infof("[QqPromotionNormDayReportLoad]: Run time:%+v", eT)
}

func (q *Qq3ReportCase) syncPullGdtAdGroupDayReport(ctx context.Context, advertiserId string, startDate string, wg *sync.WaitGroup, limiter *rate.Limiter, concurrent *semaphore.Weighted) {
    // tag: 每个子协程最好再见defer捕捉panic, 防止整个程序main协程panic
	defer func() {
		if err := recover(); err != nil {
			log.Context(ctx).Errorf("Runtime panic caught: %v\n", err)
		}
	}()
    
	defer concurrent.Release(1)
	defer wg.Done()

	page := 1
	pageSize := 100
	adgroupRepo := data.NewQqAdgroupDayReportRepo()

	for {
		// 4. 等待拿到令牌 发起请求
		err := limiter.WaitN(ctx, 1)
		if err != nil {
			log.Context(ctx).Warnf("[syncPullGdtAdGroupDayReport]#advertiserId:%s--err#%+v", advertiserId, err)
			break
		}
		log.Context(ctx).Infof("[syncPullGdtAdGroupDayReport]#advertiserId:%s--page:%d--time:%s", advertiserId, page, time.Now().Format("2006-01-02 15:04:05"))

		qqAdgroupInfo := q.queryAdGroupDayReport(ctx, advertiserId, startDate, startDate, page, pageSize)
		if len(qqAdgroupInfo.List) <= 0 {
			log.Context(ctx).Warnf("[syncPullGdtAdGroupDayReport]#advertiserId:%s, page:%d, list没有数据#%+v", advertiserId, page, qqAdgroupInfo)
			break
		}

		var unqMd5List []string
		var unqMd5MapInfo = make(map[string]response.Qq3PromotionDayRspDataItem, 0)
		// 5. 进行转换 建议所有三方接口返回的json 都用定义好的结构体进行解析接收，使用map时，因为类型断言会有问题（比如interger无法确认时int32还是int64， float32与float64）
		qqPromotionReportList := make([]response.Qq3PromotionDayRspDataItem, 0)
		mapstructure.Decode(qqAdgroupInfo.List, &qqPromotionReportList)

		for _, promotionReport := range qqPromotionReportList {

			adgroupIdStr := fmt.Sprintf("%v", promotionReport.AdgroupId)
			// strconv.FormatFloat(adgroupId, 'f', 0, 64)
			unqRemark := unit.ComputeMD5(fmt.Sprintf("%s%s%s", adgroupIdStr, "", promotionReport.Date))
			promotionReport.UnqRemark = unqRemark
			unqMd5MapInfo[unqRemark] = promotionReport

			unqMd5List = append(unqMd5List, unqRemark)
		}

		if len(unqMd5List) <= 0 {
			log.Context(ctx).Warnf("[syncPullGdtAdGroupDayReport]#advertiserId:%s, page:%d, 返回结构异常,list无合法字段字段#%+v", advertiserId, page, qqAdgroupInfo.List)
			break
		}

		// 6. tag: 查询是否已存在 获取主键id
		hasExistIdMap, err := adgroupRepo.GetUnqMd5Ids(ctx, unqMd5List)
		if err != nil {
			log.Context(ctx).Warnf("[syncPullGdtAdgroupList]#advertiserId:%s, page:%d, GetUnqMd5Ids db error#%+v", advertiserId, page, err)
		}

		err = adgroupRepo.BatchSaveQqAdGroupDayReport(ctx, hasExistIdMap, unqMd5MapInfo, advertiserId, "3")
		if err != nil {
			log.Context(ctx).Warnf("[syncPullGdtAdgroupList]#advertiserId:%s, page:%d, SaveQqAdGroupDayReport db error#%+v", advertiserId, page, err)
		}

		page++
	}

}
``` 
- 每次起协程 需要defer捕捉panic
- defer执行的时机, 禁止在循环中使用 defer 的处理

### time/rate源码分析
- reserveN 实现的
``` go
func (lim *Limiter) reserveN(t time.Time, n int, maxFutureReserve time.Duration) Reservation {
    // 获取全局锁
    // 意味着该限流器所有的请求都会争抢这个锁
    lim.mu.Lock()
    defer lim.mu.Unlock()
    ...
    // 处理 limit = Inf 和 0 的情况
    // limit=Inf 放行
    // limit=0，burst 满足则放行，否则不 ok
    ...

    // lazy 计算到目标时间 token 个数
    // 以当前的 limite 值 * 经过时间
    t, tokens := lim.advance(t)

    // 扣减请求的 N
    // Calculate the remaining number of tokens resulting from the request.
    tokens -= float64(n)

    // 如果为负，得等！计算等待时间
    // 注意：在此后，时间已经返回了。
    // 这意味着，无论是 Wait 还是 Reserve 后自行 sleep，
    // 这期间更新限流器限流值，不能更改 sleep 时间了，
    // 这可能导致带宽限流的长时间 sleep
    var waitDuration time.Duration
    if tokens < 0 {
        waitDuration = lim.limit.durationFromTokens(-tokens)
    }

    // Decide result
    ok := n <= lim.burst && waitDuration <= maxFutureReserve

    // Prepare reservation
    ...
    return r
}
```
- Wait/WaitN, Allow/AllowN, Reserve/ReserveN 实际上都是通过内部方法 
``` go
func (lim *Limiter) wait(ctx context.Context, n int, t time.Time, newTimer func(d time.Duration) (<-chan time.Time, func() bool, func())) error {
    ... 处理ctx，burst
    ... ctx 到期、N 大于 burst 报错
    ... limit 为 Inf 时直接返回通过

    // 使用 Reserve 判断
    r := lim.reserveN(t, n, waitLimit)
    if !r.ok {
        return fmt.Errorf("rate: Wait(n=%d) would exceed context deadline", n)
    }

    // ctx 和 timer 共同在 select 等待
    // 虽然此时未占用锁，但意味着不会接收 limiter 更新
    // Wait if necessary
    delay := r.DelayFrom(t)
    if delay == 0 {
        return nil
    }
    ch, stop, advance := newTimer(delay)
    defer stop()
    advance() // only has an effect when testing
    select {
    case <-ch:
        // We can proceed.
        return nil
    case <-ctx.Done():
        // Context was canceled before we could proceed.  Cancel the
        // reservation, which may permit other events to proceed sooner.
        r.Cancel()
        return ctx.Err()
    }
}
``` 
- SetLimit 会和请求共同争抢 mutex。改变 Limit/Burst 可能导致已经拿到 reserve 还未执行的请求限速不准。


``` go
func (lim *Limiter) SetLimit(newLimit Limit) {
    lim.SetLimitAt(time.Now(), newLimit)
}

func (lim *Limiter) SetLimitAt(t time.Time, newLimit Limit) {
    lim.mu.Lock()
    defer lim.mu.Unlock()

    t, tokens := lim.advance(t)

    lim.last = t
    lim.tokens = tokens
    lim.limit = newLimit
}
```
## redis版 令牌桶
- go-zero的令牌桶包 limit.TokenLimiter
![alt text](image5.png)

- Redis 中主要保存两个 key，分别是 token 数量和刷新时间。核心思想就是比较两次请求时间间隔内生成的 token 数量 + 桶内剩余 token 数量，和请求量之间的大小，如果满足则允许，否则则不允许。
``` go
// 核心结构体
type TokenLimiter struct {
   // 每秒生产速率
   rate           int
   // 桶容量
   burst          int
   store          *redis.Redis
   // redis key
   tokenKey       string
   // 桶刷新时间
   timestampKey   string

   rescueLock     sync.Mutex
   // redis是否存活
   redisAlive     uint32
   // redis监控探测任务标识
   monitorStarted bool
   // redis故障时采取进程内令牌桶限流器 本地限流用的是 golang.org/x/time/rate
   rescueLimiter  *xrate.Limiter
}

// -------------------------------------参考看门狗------//
//每秒生成token数量即token生成速度
script = `` // 以下内容
local rate = tonumber(ARGV[1])
//桶容量
local capacity = tonumber(ARGV[2])
//当前时间戳
local now = tonumber(ARGV[3])
//当前请求token数量
local requested = tonumber(ARGV[4])
//需要多少秒才能填满桶
local fill_time = capacity/rate
//向下取整,ttl为填满时间的2倍
local ttl = math.floor(fill_time*2)

//当前时间桶容量
local last_tokens = tonumber(redis.call("get", KEYS[1]))

//如果当前桶容量为0,说明是第一次进入,则默认容量为桶的最大容量
if last_tokens == nil then
last_tokens = capacity
end

//上一次刷新的时间
local last_refreshed = tonumber(redis.call("get", KEYS[2]))

//第一次进入则设置刷新时间为0
if last_refreshed == nil then
last_refreshed = 0
end

//距离上次请求的时间跨度
local delta = math.max(0, now-last_refreshed)
//距离上次请求的时间跨度,总共能生产token的数量,如果超多最大容量则丢弃多余的token
local filled_tokens = math.min(capacity, last_tokens+(delta*rate))
//本次请求token数量是否足够
local allowed = filled_tokens >= requested
//桶剩余数量
local new_tokens = filled_tokens

//允许本次token申请,计算剩余数量
if allowed then
new_tokens = filled_tokens - requested
end

//设置剩余token数量
redis.call("setex", KEYS[1], ttl, new_tokens)
//设置刷新时间
redis.call("setex", KEYS[2], ttl, now)

return allowed
// -------------------------------------参考看门狗------//

func NewTokenLimiter(rate, burst int, store *redis.Redis, key string) *TokenLimiter {
    tokenKey := fmt.Sprintf(tokenFormat, key)
    timestampKey := fmt.Sprintf(timestampFormat, key)

    return &TokenLimiter{
        rate:          rate,
        burst:         burst,
        store:         store,
        tokenKey:      tokenKey,
        timestampKey:  timestampKey,
        redisAlive:    1,
        rescueLimiter: xrate.NewLimiter(xrate.Every(time.Second/time.Duration(rate)), burst),
    }
}

func (lim *TokenLimiter) reserveN(ctx context.Context, now time.Time, n int) bool {
    // 判断 Redis 健康状态，如果 Redis 故障，则使用进程内限流器
    if atomic.LoadUint32(&lim.redisAlive) == 0 {
        return lim.rescueLimiter.AllowN(now, n)
    }

    // 执行限流脚本
    resp, err := lim.store.EvalCtx(ctx,
        script,
        []string{
            lim.tokenKey,
            lim.timestampKey,
        },
        []string{
            strconv.Itoa(lim.rate),
            strconv.Itoa(lim.burst),
            strconv.FormatInt(now.Unix(), 10),
            strconv.Itoa(n),
        })
    // redis allowed == false
    // Lua boolean false -> r Nil bulk reply
    if err == redis.Nil {
        return false
    }
    if errors.Is(err, context.DeadlineExceeded) || errors.Is(err, context.Canceled) {
        logx.Errorf("fail to use rate limiter: %s", err)
        return false
    }
    if err != nil {
        logx.Errorf("fail to use rate limiter: %s, use in-process limiter for rescue", err)
        // 如果有异常的话，会启动进程内限流 time/rate
        lim.startMonitor()
        return lim.rescueLimiter.AllowN(now, n)
    }

    code, ok := resp.(int64)
    if !ok {
        logx.Errorf("fail to eval redis script: %v, use in-process limiter for rescue", resp)
        lim.startMonitor()
        return lim.rescueLimiter.AllowN(now, n)
    }

    // redis allowed == true
    // Lua boolean true -> r integer reply with value of 1
    return code == 1
}

// 进程内限流的启动与恢复
func (lim *TokenLimiter) startMonitor() {
    lim.rescueLock.Lock()
    defer lim.rescueLock.Unlock()

    // 需要加锁保护，如果程序已经启动了，直接返回，不要重复启动
    if lim.monitorStarted {
        return
    }

    lim.monitorStarted = true
    atomic.StoreUint32(&lim.redisAlive, 0)

    go lim.waitForRedis()
}

func (lim *TokenLimiter) waitForRedis() {
    ticker := time.NewTicker(pingInterval)
    // 更新监控进程的状态
    defer func() {
        ticker.Stop()
        lim.rescueLock.Lock()
        lim.monitorStarted = false
        lim.rescueLock.Unlock()
    }()

    for range ticker.C {
        // 对 redis 进行健康监测，如果 redis 服务恢复了
        // 则更新 redisAlive 标识，并退出 goroutine
        if lim.store.Ping() {
            atomic.StoreUint32(&lim.redisAlive, 1)
            return
        }
    }
}

// 本质都是调用reserveN 参照time/rate
func (lim *TokenLimiter) Allow() bool {
    return lim.AllowN(time.Now(), 1)
}

func (lim *TokenLimiter) AllowCtx(ctx context.Context) bool {
    return lim.AllowNCtx(ctx, time.Now(), 1)
}

func (lim *TokenLimiter) AllowN(now time.Time, n int) bool {
    return lim.reserveN(context.Background(), now, n)
}

func (lim *TokenLimiter) AllowNCtx(ctx context.Context, now time.Time, n int) bool {
    return lim.reserveN(ctx, now, n)
}

``` 
# 动态限流：sentinel-golang 限流和kratos的限流

<font color="red">漏桶、令牌桶等，他们的缺点是单一限流和无差别限流。此外，系统需要先做压测，拿到一个初始的限流参考值，超过这个值才启动限流机制</font>

- BBR自适应限流保护（熔断用途 ）
{{< innerlink src="posts/tech/base6.md" >}}
