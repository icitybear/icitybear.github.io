---
title: "Go单机锁的应用-缓存击穿singleFlight包-读写操作聚合（I/O聚合优化）" #标题
date: 2024-02-26T15:24:09+08:00 #创建时间
lastmod: 2024-02-26T15:24:09+08:00 #更新时间
author: ["citybear"] #作者
categories: # 没有分类界面可以不填写
- tech
tags: # 标签
- go包
- go进阶
- go源码
- 缓存
keywords: 
- 
description: "" #描述 每个文章内容前面的展示描述
weight: # 输入1可以顶置文章，用来给文章展示排序，不填就默认按时间排序
slug: ""
draft: false # 是否为草稿
comments: true #是否展示评论 有自带的扩展成twikoo
showToc: true # 显示目录 文章侧边栏toc目录
TocOpen: true # 自动展开目录
hidemeta: false # 是否隐藏文章的元信息，如发布日期、作者等
disableShare: true # 底部不显示分享栏
showbreadcrumbs: true #顶部显示当前路径
cover:
    image: "" #图片路径：posts/tech/文章1/picture.png
    caption: "" #图片底部描述
    alt: ""
    relative: false

# reward: true # 打赏
mermaid: true #自己加的是否开启mermaid
---
# 相关知识点
{{< innerlink src="posts/tech/go25.md" >}}

- 分享过：<font color="red">[go防止缓存击穿之共享内存调用](https://note.youdao.com/s/Jz10iXm9)</font>
- singleflight包是读聚合，写聚合

# 说明
- <font color="red">作用是当有多个goroutine同时调用同一个函数的时候，只允许一个goroutine去调用这个函数，等到这个调用的goroutine返回结果的时候，再把结果返回给这几个同时调用的goroutine，这样可以减少并发调用的数量。</font>
- golang.org/x/sync/singleflight, 参数一key,参数二匿名函数。充当一个非常短暂的缓存，不需要手动作废或设置有效时间。
- Go标准库中有Singleflight的代码实现，路径为src/internal/singleflight/singleflight.go，放在了internal包中，所以没法直接使用
- <font color="red">本质使用了并发单机锁, singleflight是单点的，没法实现分布式，如果你需要的全局的分布式的优化可能Singleflight并不合适。(多个节点允许同时请求，主要减少单机的并非量打到db)</font>
- 推荐阅读该源码

# 使用demo
``` go
func main() {
  g := new(singleflight.Group)

  go func() {
    v, _, shared := g.Do("getData", func() (interface{}, error) {
      return getData(1)
    })
    fmt.Printf("one call v: %s, shared: %v\n", v.(string), shared)
  }()
  // 1的先执行了 getData函数只执行了一次(只输出了一次get data)，但是两个goroutine都拿到了结果，且结果相同都是data_1。shared为true表示多个调用共享了返回结果。
  time.Sleep(time.Millisecond * 500)

  go func() {
    v, _, shared := g.Do("getData", func() (interface{}, error) {
      return getData(2)
    })
    fmt.Printf("two call v: %s, shared: %v\n", v.(string), shared)
  }()

  time.Sleep(time.Second)
}

func getData(num int) (string, error) {
  fmt.Println("get data")
  time.Sleep(time.Second)
  return fmt.Sprintf("data_%d", num), nil
}

// 输出 
// get data
// one call v: data_1, shared: true
// two call v: data_1, shared: true
```
- <font color="red">由于singleflight是以阻塞读的方式来控制向下游请求的并发量的，在第一个goroutine请求没有返回前，所有的请求都将被阻塞。</font>极端情况下可能导致我们的程序hang住，由于连锁反应甚至导致我们整个系统挂掉。(请求越堆越多) 
  - 使用select和DoChan为函数调用设置超时时间。
  - 第一个调用超时了, 后续的请求并不会共享这个超时，而是会<font color="red">以当前goroutine的实际请求超时时间为准，并且共享了第一个调用请求的结果</font>，也就是只会调用下游函数一次。
  
``` go

func main() {
  g := new(singleflight.Group)

  go func() {
    // 超时控制
    ctx, cancel := context.WithTimeout(context.Background(), time.Millisecond*500)
    defer cancel()

    v, err, shared := getDataWithTimeout(ctx, g, 1)
    fmt.Printf("one call v: %s, shared: %v err: %v\n", v, shared, err)
  }()

  time.Sleep(time.Millisecond * 500)
  // time.Sleep(time.Millisecond * 2800)

  go func() {
    ctx, cancel := context.WithTimeout(context.Background(), time.Millisecond*500)
    defer cancel()

    v, err, shared := getDataWithTimeout(ctx, g, 2)
    fmt.Printf("two call v: %s, shared: %v err: %v\n", v, shared, err)
  }()

  time.Sleep(time.Second)
}

func getDataWithTimeout(ctx context.Context, g *singleflight.Group, num int) (string, error, bool) {
  ch := g.DoChan("getData", func() (interface{}, error) {
    return getData(num) // 共享的是getData
  })
  // 进行了超时监测ctx, 外面设置了0.5。 0.5 < 3（getData） 那第一个被监测到超时
  // 第二个以当前的goroutine的为准 2.8 + 0.5 = 3.3 > 3(第一个所话费的时间) 第二个执行时 0.3 > xxx 所以第二个执行时，流程不需要3秒
  select {
  case <-ctx.Done():
    return "", ctx.Err(), false
  case ret := <-ch:
    return ret.Val.(string), ret.Err, ret.Shared
  }
}

func getData(num int) (string, error) {
  fmt.Println("get data")
  time.Sleep(time.Second * 3)
  return fmt.Sprintf("data_%d", num), nil
}

// get data
// one call v: , shared: false err: context deadline exceeded
// two call v: , shared: false err: context deadline exceeded

// 第一个调用超时了，后续所有的调用都会共享这个超时吗？
//  time.Sleep(time.Millisecond * 500)  改为   time.Sleep(time.Millisecond * 2800)

// 0.5 + 2.8 > 3
// get data
// one call v: , shared: false err: context deadline exceeded
// two call v: data_1, shared: true err: <nil>
```

# 实现原理

``` go
type Group struct {
  mu sync.Mutex       // Mutext用来保证并发时的读写安全
  m  map[string]*call // 用来保存同一个key的正在处理（in flight）的请求
}

// call对象，call代表了正在执行的fn函数的请求或者是已经执行完成的请求。
type call struct {
  wg sync.WaitGroup

  val interface{}
  err error

  dups  int
  chans []chan<- Result
}

```
- DoChan方法和Do方法类似, doCall方法会实际的执行函数fn
``` go

func (g *Group) Do(key string, fn func() (interface{}, error)) (v interface{}, err error, shared bool) {
  g.mu.Lock()
  if g.m == nil {
    g.m = make(map[string]*call)
  }
  if c, ok := g.m[key]; ok { // 存在相同的key
    c.dups++
    g.mu.Unlock()
    c.wg.Wait() // 等待这个key的第一个调用完成

    // ...
    
    return c.val, c.err, true // 复用第一个key的请求结果
  }
  c := new(call) // 第一个调用创建一个call
  c.wg.Add(1)
  g.m[key] = c
  g.mu.Unlock()

  g.doCall(c, key, fn) // 调用方法
  return c.val, c.err, c.dups > 0
}

// doCall方法会实际的执行函数fn
func (g *Group) doCall(c *call, key string, fn func() (interface{}, error)) {
  defer func() {
    // ...
    
    g.mu.Lock()
    defer g.mu.Unlock()
    c.wg.Done() // 阻塞等待完成
    if g.m[key] == c {
      delete(g.m, key) // 调用完成删除这个key
    }
    
    // ...
  }()

  func() {
    // ...
    
    c.val, c.err = fn() // 真正调用，结果赋值给call
    
    // ...
  }()
}
```

# 应用场景

缓存击穿是指针对某个访问非常频繁的热点数据的请求，没有命中缓存，所以大量请求全部打到数据库，导致数据库压力激增，会影响数据库处理其他请求，缓存击穿的情况经常发生在热点数据过期时
- 热点数据不多且为提前预知的话，我们可以提前把热点数据加载到缓存中，就不设置过期时间了
- 热点数据是提前不知道的，比如突发的热点新闻。还有的数据当时是热点，可能很快就变成了冷数据。针对类似的场景是不太合适设置不过期的
- 伪代码
``` go
func getDataSingleFlight(key string) (interface{}, error) {
  v, err, _ := g.Do(key, func() (interface{}, error) {
    // 查缓存
    data, err := getDataFromCache(key)
    if err == nil {
      return data, nil
    }
    if errors.Is(err, ErrNotFound) {
      // 查DB
      data, err := getDataFromDB(key)
      if err == nil {
        setCache(data) // 设置缓存
        return data, nil
      }
      return nil, err
    }
    return nil, err // 缓存出错直接返回，防止穿透到DB
  })
  if err != nil {
    return nil, err
  }
  
  return v, nil
}

```

# 写聚合
[I/O聚合优化 图解](https://www.51cto.com/article/782947.html)